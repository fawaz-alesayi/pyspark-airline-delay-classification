{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Data set from Kaggle https://www.kaggle.com/datasets/sherrytp/airline-delay-analysis\n",
    "\n",
    "## Description\n",
    "\n",
    "### Context\n",
    "The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. I came across this useful data from DOT's database at working and figured this would be a really helpful dataset: Summary information on the number of on-time, delayed, canceled, and diverted flight.\n",
    "\n",
    "### Content\n",
    "The datasets contain daily airline information covering from flight information, carrier company, to taxing-in, taxing-out time, and generalized delay reason of exactly 10 years, from 2009 to 2019. The DOT's database is renewed from 2018, so there might be a minor change in the column names.\n",
    "\n",
    "### Schema\n",
    "- |-- FL_DATE: string (nullable = true) - Flight date\n",
    "- |-- OP_CARRIER: string (nullable = true) - Carrier operating the flight\n",
    "- |-- OP_CARRIER_FL_NUM: integer (nullable = true) - Flight number\n",
    "- |-- ORIGIN: string (nullable = true) - Origin airport\n",
    "- |-- DEST: string (nullable = true) - Destination airport\n",
    "- |-- CRS_DEP_TIME: integer (nullable = true) - Scheduled departure time\n",
    "- |-- DEP_TIME: double (nullable = true) - Actual departure time\n",
    "- |-- DEP_DELAY: double (nullable = true) - Departure delay\n",
    "- |-- TAXI_OUT: double (nullable = true) - Taxi out time\n",
    "- |-- WHEELS_OFF: double (nullable = true) - Wheels off time\n",
    "- |-- WHEELS_ON: double (nullable = true) - Wheels on time\n",
    "- |-- TAXI_IN: double (nullable = true) - Taxi in time\n",
    "- |-- CRS_ARR_TIME: integer (nullable = true) - Scheduled arrival time\n",
    "- |-- ARR_TIME: double (nullable = true) - Actual arrival time\n",
    "- |-- ARR_DELAY: double (nullable = true) - Arrival delay\n",
    "- |-- CANCELLED: double (nullable = true) - Cancelled flight\n",
    "- |-- CANCELLATION_CODE: string (nullable = true) - Cancellation code\n",
    "- |-- DIVERTED: double (nullable = true) - Diverted flight\n",
    "- |-- CRS_ELAPSED_TIME: double (nullable = true) - Scheduled elapsed time\n",
    "- |-- ACTUAL_ELAPSED_TIME: double (nullable = true) - Actual elapsed time\n",
    "- |-- AIR_TIME: double (nullable = true) - Air time\n",
    "- |-- DISTANCE: double (nullable = true) - Distance to destination\n",
    "- |-- CARRIER_DELAY: double (nullable = true) - Carrier delay\n",
    "- |-- WEATHER_DELAY: double (nullable = true) - Weather delay\n",
    "- |-- NAS_DELAY: double (nullable = true) - NAS delay\n",
    "- |-- SECURITY_DELAY: double (nullable = true) - Security delay\n",
    "- |-- LATE_AIRCRAFT_DELAY: double (nullable = true) - Late aircraft delay\n",
    "- |-- Unnamed: 27: string (nullable = true) - Unknown\n",
    "\n",
    "### Goals\n",
    "Predict the departure delay of a flight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2018.csv.zip to /workspace/pyspark-airline-delay-classification\n",
      " 98%|███████████████████████████████████████ | 233M/238M [00:05<00:00, 46.0MB/s]\n",
      "100%|████████████████████████████████████████| 238M/238M [00:05<00:00, 43.4MB/s]\n",
      "Archive:  2018.csv.zip\n",
      "  inflating: 2018.csv                \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d sherrytp/airline-delay-analysis -f \"airline delay analysis/2018.csv\"\n",
    "!unzip 2018.csv.zip\n",
    "!rm 2018.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3435m\n",
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3435m\n",
      "22/05/14 15:17:35 WARN Utils: Your hostname, fawazalesay-pysparkairl-mlxxsdnyoem resolves to a loopback address: 127.0.0.1; using 10.0.5.2 instead (on interface ceth0)\n",
      "22/05/14 15:17:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/14 15:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/14 15:17:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/14 15:17:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# initlize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"airline-delay-regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make Jupyter work with Gitpod\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the data into a dataframe and print the schema\n",
    "df = spark.read.csv(\"2018.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We dropped all the columns that are not needed for the analysis or that would be considered cheating.\n",
    "\n",
    "We also added DAY, MONTH, and YEAR columns to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------------+---------+------------+----------------+--------+----+-----+---+\n",
      "|OP_CARRIER|ORIGIN|DEST|CRS_DEP_TIME|DEP_DELAY|CRS_ARR_TIME|CRS_ELAPSED_TIME|DISTANCE|YEAR|MONTH|DAY|\n",
      "+----------+------+----+------------+---------+------------+----------------+--------+----+-----+---+\n",
      "|        UA|   EWR| DEN|        1517|     -5.0|        1745|           268.0|  1605.0|2018|    1|  1|\n",
      "|        UA|   LAS| SFO|        1115|     -8.0|        1254|            99.0|   414.0|2018|    1|  1|\n",
      "|        UA|   SNA| DEN|        1335|     -5.0|        1649|           134.0|   846.0|2018|    1|  1|\n",
      "|        UA|   RSW| ORD|        1546|      6.0|        1756|           190.0|  1120.0|2018|    1|  1|\n",
      "|        UA|   ORD| ALB|         630|     20.0|         922|           112.0|   723.0|2018|    1|  1|\n",
      "+----------+------+----+------------+---------+------------+----------------+--------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7096202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "df = df.drop(\"Unnamed: 27\", \"LATE_AIRCRAFT_DELAY\", \"SECURITY_DELAY\", \"NAS_DELAY\", \"WEATHER_DELAY\", \"CARRIER_DELAY\", \"AIR_TIME\", \"ACTUAL_ELAPSED_TIME\", \"DIVERTED\", \"CANCELLATION_CODE\", \"CANCELLED\", \"ARR_TIME\", \"TAXI_IN\", \"WHEELS_ON\", \"WHEELS_OFF\", \"TAXI_OUT\", \"DEP_TIME\", \"OP_CARRIER_FL_NUM\", \"ARR_DELAY\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_month(date):\n",
    "    return int(date.split(\"-\")[1])\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_day(date):\n",
    "    return int(date.split(\"-\")[2])\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_year(date):\n",
    "    return int(date.split(\"-\")[0])\n",
    "\n",
    "# Adds month and a day column to the dataframe\n",
    "df = df.withColumn(\"YEAR\", get_year(df[\"FL_DATE\"]).cast(IntegerType()))\n",
    "df = df.withColumn(\"MONTH\", get_month(df[\"FL_DATE\"]).cast(IntegerType()))\n",
    "df = df.withColumn(\"DAY\", get_day(df[\"FL_DATE\"]).cast(IntegerType()))\n",
    "\n",
    "df = df.drop(\"FL_DATE\")\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.show(5)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for training\n",
    "\n",
    "We use StringIndexer to convert the categorical variables to numerical values.\n",
    "\n",
    "We use OneHotEncoder to convert the numerical values (in ordinal form) to nominal values.\n",
    "\n",
    "We use VectorAssembler to convert the numerical variables to a feature vector (required by the model).\n",
    "\n",
    "We normalize the feature vector using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max categories: 358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Encode the categorical features using StringIndexer\n",
    "indexer = StringIndexer(inputCols=[\"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"DAY\", \"MONTH\"], outputCols=[\"OP_CARRIER_INDEX\", \"ORIGIN_INDEX\", \"DEST_INDEX\", \"DAY_INDEX\", \"MONTH_INDEX\"])\n",
    "\n",
    "# Use one hot encoding to encode the categorical features\n",
    "encoder = OneHotEncoder(inputCols=[\"OP_CARRIER_INDEX\", \"ORIGIN_INDEX\", \"DEST_INDEX\", \"DAY_INDEX\", \"MONTH_INDEX\"], outputCols=[\"OP_CARRIER_VEC\", \"ORIGIN_VEC\", \"DEST_VEC\", \"DAY_VEC\", \"MONTH_VEC\"])\n",
    "\n",
    "# Create the assembler\n",
    "assembler = VectorAssembler(inputCols=[\"OP_CARRIER_VEC\", \"ORIGIN_VEC\", \"DEST_VEC\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"DAY_VEC\", \"MONTH_VEC\", \"YEAR\"], outputCol=\"features\")\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Get the maximum number of all categorical features in the dataframe\n",
    "num_of_origins = df.select(\"ORIGIN\").distinct().count()\n",
    "num_of_destinations = df.select(\"DEST\").distinct().count()\n",
    "num_of_carriers = df.select(\"OP_CARRIER\").distinct().count()\n",
    "max_num_of_categorical_features = max(num_of_origins, num_of_destinations, num_of_carriers)\n",
    "print(\"max categories:\", max_num_of_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 15:22:12 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/14 15:22:12 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/05/14 15:22:29 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "[Stage 46:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 44.57553449188212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"DEP_DELAY\", regParam=0.3)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Print the RMSE\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"DEP_DELAY\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.93221958518358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std: 44.739548581249586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", test.select(\"DEP_DELAY\").agg({\"DEP_DELAY\": \"mean\"}).collect()[0][0])\n",
    "print(\"Std:\", test.select(\"DEP_DELAY\").agg({\"DEP_DELAY\": \"stddev\"}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model: Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 15:30:49 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 70.7 MiB so far)\n",
      "22/05/14 15:30:49 WARN BlockManager: Persisting block rdd_221_2 to disk instead.\n",
      "22/05/14 15:30:49 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 46.7 MiB so far)\n",
      "22/05/14 15:30:49 WARN BlockManager: Persisting block rdd_221_3 to disk instead.\n",
      "22/05/14 15:30:49 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 46.7 MiB so far)\n",
      "22/05/14 15:30:49 WARN BlockManager: Persisting block rdd_221_4 to disk instead.\n",
      "22/05/14 15:30:51 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 109.6 MiB so far)\n",
      "22/05/14 15:30:51 WARN BlockManager: Persisting block rdd_221_1 to disk instead.\n",
      "22/05/14 15:30:54 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 167.4 MiB so far)\n",
      "22/05/14 15:30:54 WARN BlockManager: Persisting block rdd_221_5 to disk instead.\n",
      "22/05/14 15:30:54 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 167.4 MiB so far)\n",
      "22/05/14 15:30:54 WARN BlockManager: Persisting block rdd_221_0 to disk instead.\n",
      "22/05/14 15:32:31 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:32:31 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 110.2 MiB so far)\n",
      "22/05/14 15:32:31 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 13.6 MiB so far)\n",
      "22/05/14 15:32:31 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 252.8 MiB so far)\n",
      "22/05/14 15:32:31 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 110.2 MiB so far)\n",
      "22/05/14 15:32:32 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 379.3 MiB so far)\n",
      "22/05/14 15:32:52 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 251.1 MiB so far)\n",
      "22/05/14 15:32:52 WARN BlockManager: Persisting block rdd_221_6 to disk instead.\n",
      "22/05/14 15:33:07 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 379.3 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:11 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:20 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 379.3 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:23 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:29 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 379.3 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 46.8 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 46.8 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:32 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 110.2 MiB so far)\n",
      "22/05/14 15:33:38 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 379.3 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_0 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_1 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_2 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_4 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_3 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:41 WARN MemoryStore: Not enough space to cache rdd_221_5 in memory! (computed 71.1 MiB so far)\n",
      "22/05/14 15:33:46 WARN MemoryStore: Not enough space to cache rdd_221_6 in memory! (computed 379.3 MiB so far)\n",
      "[Stage 68:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Decision Tree Regression): 44.77400386137411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression Model\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=\"DEP_DELAY\", maxDepth=5, maxBins=max_num_of_categorical_features)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler, dtr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Print the RMSE\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"DEP_DELAY\", metricName=\"rmse\")\n",
    "print(\"RMSE (Decision Tree Regression):\", evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
