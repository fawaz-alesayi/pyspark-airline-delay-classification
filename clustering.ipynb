{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "memory = '8g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install shapely to deal with geospatial data\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d dhruvildave/ookla-internet-speed-dataset -f \"2020-q2/2020-04-01_performance_mobile_tiles.parquet\"\n",
    "!unzip 2020-04-01_performance_mobile_tiles.parquet.zip\n",
    "!rm 2020-04-01_performance_mobile_tiles.parquet.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3435m\n",
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3435m\n",
      "22/05/14 11:21:24 WARN Utils: Your hostname, fawazalesay-pysparkairl-mlxxsdnyoem resolves to a loopback address: 127.0.0.1; using 10.0.5.2 instead (on interface ceth0)\n",
      "22/05/14 11:21:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/14 11:21:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/14 11:21:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.id', 'driver'), ('spark.sql.warehouse.dir', 'file:/workspace/pyspark-airline-delay-classification/spark-warehouse'), ('spark.app.id', 'local-1652527285957'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1652527285134'), ('spark.driver.memory', '8g'), ('spark.app.name', 'internet-analysis-clustering'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.port', '35211'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.host', '10.0.5.2'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "# initlize pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"internet-analysis-clustering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make Jupyter work with Gitpod\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- quadkey: string (nullable = true)\n",
      " |-- tile: string (nullable = true)\n",
      " |-- avg_d_kbps: long (nullable = true)\n",
      " |-- avg_u_kbps: long (nullable = true)\n",
      " |-- avg_lat_ms: long (nullable = true)\n",
      " |-- tests: long (nullable = true)\n",
      " |-- devices: long (nullable = true)\n",
      "\n",
      "+----------------+--------------------+----------+----------+----------+-----+-------+\n",
      "|         quadkey|                tile|avg_d_kbps|avg_u_kbps|avg_lat_ms|tests|devices|\n",
      "+----------------+--------------------+----------+----------+----------+-----+-------+\n",
      "|1203022122320032|POLYGON((24.09301...|     28772|      3165|        34|    8|      1|\n",
      "|0313113213321131|POLYGON((-1.49963...|     20782|     10180|        54|    2|      2|\n",
      "|1221210331312333|POLYGON((30.88806...|     22690|     22416|       449|    6|      2|\n",
      "|1200312211223323|POLYGON((18.00109...|     54493|      4635|        21|    2|      2|\n",
      "|0302233220203221|POLYGON((-81.5130...|     90669|      6576|        21|    1|      1|\n",
      "+----------------+--------------------+----------+----------+----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a dataframe and print the schema\n",
    "df = spark.read.parquet(\"2020-04-01_performance_mobile_tiles.parquet\")\n",
    "df.printSchema()\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def longitude(polygon: str):\n",
    "    return wkt.loads(polygon).centroid.x\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def latitude(polygon):\n",
    "    return wkt.loads(polygon).centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+----------+----------+-----+-------+------------------+------------------+\n",
      "|         quadkey|                tile|avg_d_kbps|avg_u_kbps|avg_lat_ms|tests|devices|         longitude|          latitude|\n",
      "+----------------+--------------------+----------+----------+----------+-----+-------+------------------+------------------+\n",
      "|1203022122320032|POLYGON((24.09301...|     28772|      3165|        34|    8|      1| 24.09576416015625| 49.88224742799456|\n",
      "|0313113213321131|POLYGON((-1.49963...|     20782|     10180|        54|    2|      2| -1.49688720703125|52.953602268373295|\n",
      "|1221210331312333|POLYGON((30.88806...|     22690|     22416|       449|    6|      2| 30.89080810546875|29.919232776382895|\n",
      "|1200312211223323|POLYGON((18.00109...|     54493|      4635|        21|    2|      2| 18.00384521484375|59.356996008027856|\n",
      "|0302233220203221|POLYGON((-81.5130...|     90669|      6576|        21|    1|      1|-81.51031494140625|41.317012752730506|\n",
      "+----------------+--------------------+----------+----------+----------+-----+-------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- quadkey: string (nullable = true)\n",
      " |-- tile: string (nullable = true)\n",
      " |-- avg_d_kbps: long (nullable = true)\n",
      " |-- avg_u_kbps: long (nullable = true)\n",
      " |-- avg_lat_ms: long (nullable = true)\n",
      " |-- tests: long (nullable = true)\n",
      " |-- devices: long (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Adds two columns: longitude and latitude\n",
    "df = df.withColumn(\"longitude\", longitude(df.tile))\n",
    "df = df.withColumn(\"latitude\", latitude(df.tile))\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- avg_d_kbps: long (nullable = true)\n",
      " |-- avg_u_kbps: long (nullable = true)\n",
      " |-- avg_lat_ms: long (nullable = true)\n",
      " |-- tests: long (nullable = true)\n",
      " |-- devices: long (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the \"quadkey\" and \"tile\" columns\n",
    "df = df.drop(\"tile\")\n",
    "df = df.drop(\"quadkey\")\n",
    "df.printSchema()\n",
    "\n",
    "# Drop null values\n",
    "df = df.dropna()\n",
    "\n",
    "# We will use only half of the data.\n",
    "df = df.sample(fraction=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "As usual, we generate the features vector using VectorAssembler\n",
    "\n",
    "Then we apply MinMax Normalization using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"avg_d_kbps\", \"avg_u_kbps\", \"tests\", \"devices\", \"latitude\", \"longitude\"], outputCol=\"features\")\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 11:22:26 ERROR Instrumentation: java.lang.IllegalArgumentException: scaledFeatures does not exist. Available: avg_d_kbps, avg_u_kbps, avg_lat_ms, tests, devices, longitude, latitude, features\n",
      "\tat org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)\n",
      "\tat scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)\n",
      "\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:277)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnTypes(SchemaUtils.scala:59)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.validateVectorCompatibleColumn(SchemaUtils.scala:205)\n",
      "\tat org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema(KMeans.scala:98)\n",
      "\tat org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema$(KMeans.scala:97)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:272)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:372)\n",
      "\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:330)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "scaledFeatures does not exist. Available: avg_d_kbps, avg_u_kbps, avg_lat_ms, tests, devices, longitude, latitude, features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/workspace/pyspark-airline-delay-classification/clustering.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfawazalesay-pysparkairl-mlxxsdnyoem/workspace/pyspark-airline-delay-classification/clustering.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(featuresCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mscaledFeatures\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msetK(\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfawazalesay-pysparkairl-mlxxsdnyoem/workspace/pyspark-airline-delay-classification/clustering.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39m[assembler, kmeans])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfawazalesay-pysparkairl-mlxxsdnyoem/workspace/pyspark-airline-delay-classification/clustering.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(df)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfawazalesay-pysparkairl-mlxxsdnyoem/workspace/pyspark-airline-delay-classification/clustering.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfawazalesay-pysparkairl-mlxxsdnyoem/workspace/pyspark-airline-delay-classification/clustering.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py?line=111'>112</a>\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py?line=113'>114</a>\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py?line=114'>115</a>\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/pipeline.py?line=115'>116</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=334'>335</a>\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=335'>336</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=336'>337</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=318'>319</a>\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=319'>320</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=328'>329</a>\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=329'>330</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=330'>331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/ml/wrapper.py?line=331'>332</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: scaledFeatures does not exist. Available: avg_d_kbps, avg_u_kbps, avg_lat_ms, tests, devices, longitude, latitude, features"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\").setK(3)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Evaluate clustering by computing Silhouett e score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "print(\"Silhouette with squared euclidean distance = \", evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
